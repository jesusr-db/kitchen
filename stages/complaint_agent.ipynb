{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "#### complaint agent\n",
    "\n",
    "Builds and ships an order-complaint agent: author tools, assemble the LangGraph workflow, evaluate it, and promote the packaged model into production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "#### Tool & View Registration\n",
    "\n",
    "- `CREATE SCHEMA` guarantees the shared `${CATALOG}.ai` workspace exists for agent assets.\n",
    "- `order_delivery_times_per_location_view` summarizes delivery percentiles per brand/location.\n",
    "- `get_order_overview(oid)` returns structured order metadata, items, and customer info.\n",
    "- `get_order_timing(oid)` exposes created/delivered timestamps plus transit duration.\n",
    "- `get_location_timings(loc)` yields P50/P75/P99 delivery benchmarks for benchmarking complaints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS ${CATALOG}.ai;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tdy6yy3gheg",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW ${CATALOG}.ai.order_delivery_times_per_location_view AS\n",
    "WITH order_times AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    MAX(CASE WHEN event_type = 'order_created' THEN try_to_timestamp(ts) END) AS order_created_time,\n",
    "    MAX(CASE WHEN event_type = 'delivered' THEN try_to_timestamp(ts) END) AS delivered_time\n",
    "  FROM\n",
    "    ${CATALOG}.lakeflow.all_events\n",
    "  WHERE\n",
    "    try_to_timestamp(ts) >= CURRENT_TIMESTAMP() - INTERVAL 1 DAY\n",
    "  GROUP BY\n",
    "    order_id,\n",
    "    location\n",
    "),\n",
    "total_order_times AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    (UNIX_TIMESTAMP(delivered_time) - UNIX_TIMESTAMP(order_created_time)) / 60 AS total_order_time_minutes\n",
    "  FROM\n",
    "    order_times\n",
    "  WHERE\n",
    "    order_created_time IS NOT NULL\n",
    "    AND delivered_time IS NOT NULL\n",
    ")\n",
    "SELECT\n",
    "  location,\n",
    "  PERCENTILE(total_order_time_minutes, 0.50) AS P50,\n",
    "  PERCENTILE(total_order_time_minutes, 0.75) AS P75,\n",
    "  PERCENTILE(total_order_time_minutes, 0.99) AS P99\n",
    "FROM\n",
    "  total_order_times\n",
    "GROUP BY\n",
    "  location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_order_overview(oid STRING COMMENT 'The unique order identifier to retrieve information for')\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  location STRING COMMENT 'Order location',\n",
    "  items_json STRING COMMENT 'JSON array of ordered items with details',\n",
    "  customer_address STRING COMMENT 'Customer delivery address',\n",
    "  brand_id BIGINT COMMENT 'Brand ID for the order',\n",
    "  order_created_ts TIMESTAMP COMMENT 'When the order was created'\n",
    ")\n",
    "COMMENT 'Returns basic order information including items, location, and customer details'\n",
    "RETURN\n",
    "  WITH order_created_events AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      location,\n",
    "      get_json_object(body, '$.items') as items_json,\n",
    "      get_json_object(body, '$.customer_addr') as customer_address,\n",
    "      -- Extract brand_id from first item in the order\n",
    "      CAST(get_json_object(get_json_object(body, '$.items[0]'), '$.brand_id') AS BIGINT) as brand_id,\n",
    "      try_to_timestamp(ts) as order_created_ts\n",
    "    FROM ${CATALOG}.lakeflow.all_events\n",
    "    WHERE order_id = oid AND event_type = 'order_created'\n",
    "    LIMIT 1\n",
    "  )\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    items_json,\n",
    "    customer_address,\n",
    "    brand_id,\n",
    "    order_created_ts\n",
    "  FROM order_created_events;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_order_timing(oid STRING COMMENT 'The unique order identifier to get timing information for')\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  order_created_ts TIMESTAMP COMMENT 'When the order was created',\n",
    "  delivered_ts TIMESTAMP COMMENT 'When the order was delivered (NULL if not delivered)',\n",
    "  delivery_duration_minutes FLOAT COMMENT 'Time from order creation to delivery in minutes (NULL if not delivered)',\n",
    "  delivery_status STRING COMMENT 'Current delivery status: delivered, in_progress, or unknown'\n",
    ")\n",
    "COMMENT 'Returns timing information for a specific order'\n",
    "RETURN\n",
    "  WITH order_events AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      event_type,\n",
    "      try_to_timestamp(ts) as event_ts\n",
    "    FROM ${CATALOG}.lakeflow.all_events\n",
    "    WHERE order_id = oid\n",
    "  ),\n",
    "  timing_summary AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      MIN(CASE WHEN event_type = 'order_created' THEN event_ts END) as order_created_ts,\n",
    "      MAX(CASE WHEN event_type = 'delivered' THEN event_ts END) as delivered_ts\n",
    "    FROM order_events\n",
    "    GROUP BY order_id\n",
    "  )\n",
    "  SELECT\n",
    "    order_id,\n",
    "    order_created_ts,\n",
    "    delivered_ts,\n",
    "    CASE\n",
    "      WHEN delivered_ts IS NOT NULL AND order_created_ts IS NOT NULL THEN\n",
    "        CAST((UNIX_TIMESTAMP(delivered_ts) - UNIX_TIMESTAMP(order_created_ts)) / 60 AS FLOAT)\n",
    "      ELSE NULL\n",
    "    END as delivery_duration_minutes,\n",
    "    CASE\n",
    "      WHEN delivered_ts IS NOT NULL THEN 'delivered'\n",
    "      WHEN order_created_ts IS NOT NULL THEN 'in_progress'\n",
    "      ELSE 'unknown'\n",
    "    END as delivery_status\n",
    "  FROM timing_summary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_location_timings(loc STRING COMMENT 'Location name as a string')\n",
    "RETURNS TABLE (\n",
    "  location STRING COMMENT 'Location of the order source',\n",
    "  P50 FLOAT COMMENT '50th percentile delivery time in minutes',\n",
    "  P75 FLOAT COMMENT '75th percentile delivery time in minutes',\n",
    "  P99 FLOAT COMMENT '99th percentile delivery time in minutes'\n",
    ")\n",
    "COMMENT 'Returns the 50/75/99th percentile of delivery times for a location to benchmark order timing'\n",
    "RETURN\n",
    "  SELECT location, P50, P75, P99\n",
    "  FROM ${CATALOG}.ai.order_delivery_times_per_location_view AS odlt\n",
    "  WHERE odlt.location = loc;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dq5ml4wp6v",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "- Install LangGraph, Databricks agent packages, and restart Python for a clean runtime.\n",
    "- Capture widget inputs (`CATALOG`, `LLM_MODEL`) and create an MLflow dev experiment for trace logging.\n",
    "- Define a templated `%%writefilev` magic that emits files with notebook variable substitution.\n",
    "- Materialize `agent.py` containing the LangGraph complaint workflow wired to UC SQL tools and the chosen LLM endpoint.\n",
    "- Pull a delivered `order_id` sample and build the MLflow model signature/resources for logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3tu3r2gso",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow-skinny[databricks] langgraph==0.3.4 databricks-langchain databricks-agents uv\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dl04kgwv9ap",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "LLM_MODEL = dbutils.widgets.get(\"LLM_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ktoahik8tl",
   "metadata": {},
   "outputs": [],
   "source": "import mlflow\n\n# Create/set dev experiment for development and evaluation traces\n# Use shared path for job compatibility\ndev_experiment_name = f\"/Shared/{CATALOG}_complaint_agent_dev\"\n\n# set_experiment creates the experiment if it doesn't exist, or activates it if it does\ndev_experiment = mlflow.set_experiment(dev_experiment_name)\ndev_experiment_id = dev_experiment.experiment_id\nprint(f\"✅ Using dev experiment: {dev_experiment_name} (ID: {dev_experiment_id})\")\n\n# Add experiment to UC state for cleanup\nimport sys\nsys.path.append('../utils')\nfrom uc_state import add\n\nexperiment_data = {\n    \"experiment_id\": dev_experiment_id,\n    \"name\": dev_experiment_name\n}\nadd(CATALOG, \"experiments\", experiment_data)\nprint(f\"✅ Added dev experiment to UC state\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bruu85upqq5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def writefilev(line, cell):\n",
    "    \"\"\"\n",
    "    %%writefilev file.py\n",
    "    Allows {{var}} substitutions while leaving normal {} intact.\n",
    "    \"\"\"\n",
    "    filename = line.strip()\n",
    "\n",
    "    def replacer(match):\n",
    "        expr = match.group(1)\n",
    "        return str(eval(expr, globals(), locals()))\n",
    "\n",
    "    # Replace only double braces {{var}}\n",
    "    content = re.sub(r\"\\{\\{(.*?)\\}\\}\", replacer, cell)\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Wrote file with substitutions: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w2c70l4ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefilev agent.py\n",
    "import json\n",
    "from typing import Annotated, Any, Generator, Optional, Sequence, TypedDict, Union, Literal, cast\n",
    "from uuid import uuid4\n",
    "\n",
    "import mlflow\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    ")\n",
    "from langchain_core.messages.utils import convert_to_messages\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "LLM_MODEL = \"{{LLM_MODEL}}\"\n",
    "CATALOG = \"{{CATALOG}}\"\n",
    "\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "\n",
    "class ComplaintResponse(TypedDict):\n",
    "    order_id: str\n",
    "    complaint_category: Literal[\"delivery_delay\", \"missing_items\", \"food_quality\", \"service_issue\", \"billing\", \"other\"]\n",
    "    decision: Literal[\"auto_credit\", \"investigate\", \"escalate\"]\n",
    "    credit_amount: float\n",
    "    rationale: str\n",
    "    customer_response: str\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "\n",
    "LLM_ENDPOINT_NAME = f\"{LLM_MODEL}\"\n",
    "base_llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "system_prompt = \"\"\"You are ComplaintAgent for Casper's ghost kitchen.\n",
    "\n",
    "Process:\n",
    "1. Extract order_id from complaint\n",
    "2. Call get_order_overview(order_id)\n",
    "3. Call get_order_timing(order_id)\n",
    "4. If timing complaint, call get_location_timings(location)\n",
    "5. Make decision\n",
    "\n",
    "Decisions:\n",
    "- AUTO-CREDIT: Clear minor issues (late >P75)\n",
    "- INVESTIGATE: Moderate issues (food quality, missing items)\n",
    "- ESCALATE: Severe issues (legal, safety)\n",
    "\n",
    "Return JSON with: order_id, complaint_category, decision, credit_amount, rationale, customer_response\"\"\"\n",
    "\n",
    "tools: list[BaseTool] = []\n",
    "uc_tool_names = [\n",
    "    f\"{CATALOG}.ai.get_order_overview\",\n",
    "    f\"{CATALOG}.ai.get_order_timing\",\n",
    "    f\"{CATALOG}.ai.get_location_timings\",\n",
    "]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools.extend(uc_toolkit.tools)\n",
    "\n",
    "RESPONSE_FIELDS = {\n",
    "    \"order_id\",\n",
    "    \"complaint_category\",\n",
    "    \"decision\",\n",
    "    \"credit_amount\",\n",
    "    \"rationale\",\n",
    "    \"customer_response\",\n",
    "}\n",
    "\n",
    "def parse_structured_response(obj: Union[AIMessage, dict[str, Any]]) -> ComplaintResponse:\n",
    "    \"\"\"Coerce an AIMessage or dict into the ComplaintResponse schema.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        candidate = obj\n",
    "    else:\n",
    "        parsed = obj.additional_kwargs.get(\"parsed_structured_output\")\n",
    "        if isinstance(parsed, dict):\n",
    "            candidate = parsed\n",
    "        else:\n",
    "            content = obj.content\n",
    "            if isinstance(content, str):\n",
    "                raw = content\n",
    "            elif isinstance(content, list):\n",
    "                raw = \"\".join(part.get(\"text\", \"\") if isinstance(part, dict) else str(part) for part in content)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported message content type for structured output\")\n",
    "            candidate = json.loads(raw)\n",
    "\n",
    "    missing = RESPONSE_FIELDS.difference(candidate.keys())\n",
    "    if missing:\n",
    "        raise ValueError(f\"Structured response missing fields: {sorted(missing)}\")\n",
    "\n",
    "    return cast(ComplaintResponse, candidate)\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    "):\n",
    "    tool_model = model.bind_tools(tools, tool_choice=\"auto\")\n",
    "    structured_model = tool_model.with_structured_output(ComplaintResponse)\n",
    "\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            return \"continue\"\n",
    "        return \"end\"\n",
    "\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "\n",
    "    tool_runnable = preprocessor | tool_model\n",
    "    structured_runnable = preprocessor | structured_model\n",
    "\n",
    "    def call_model(state: AgentState, config: RunnableConfig):\n",
    "        response = tool_runnable.invoke(state, config)\n",
    "        if not isinstance(response, AIMessage):\n",
    "            raise ValueError(f\"Expected AIMessage from model, received {type(response)}\")\n",
    "\n",
    "        if response.tool_calls:\n",
    "            return {\"messages\": [response]}\n",
    "\n",
    "        try:\n",
    "            parsed = parse_structured_response(response)\n",
    "        except (json.JSONDecodeError, ValueError):\n",
    "            structured = structured_runnable.invoke(state, config)\n",
    "            parsed = parse_structured_response(structured)\n",
    "\n",
    "        structured_message = AIMessage(\n",
    "            id=response.id or str(uuid4()),\n",
    "            content=json.dumps(parsed),\n",
    "            additional_kwargs={\"parsed_structured_output\": parsed},\n",
    "        )\n",
    "        return {\"messages\": [structured_message]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\"continue\": \"tools\", \"end\": END},\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    def _langchain_to_responses(self, messages: list[BaseMessage]) -> list[dict[str, Any]]:\n",
    "        \"\"\"Convert LangChain messages to Responses output items.\"\"\"\n",
    "        items: list[dict[str, Any]] = []\n",
    "\n",
    "        for raw_message in messages or []:\n",
    "            message = raw_message.model_dump() if hasattr(raw_message, \"model_dump\") else raw_message\n",
    "            role = message.get(\"type\")\n",
    "\n",
    "            if role == \"ai\":\n",
    "                if tool_calls := message.get(\"tool_calls\"):\n",
    "                    for tool_call in tool_calls:\n",
    "                        items.append(\n",
    "                            self.create_function_call_item(\n",
    "                                id=message.get(\"id\") or str(uuid4()),\n",
    "                                call_id=tool_call[\"id\"],\n",
    "                                name=tool_call[\"name\"],\n",
    "                                arguments=json.dumps(tool_call.get(\"args\", {})),\n",
    "                            )\n",
    "                        )\n",
    "                    continue\n",
    "\n",
    "                content = message.get(\"content\")\n",
    "                if isinstance(content, list):\n",
    "                    text_content = \"\".join(ch.get(\"text\", \"\") if isinstance(ch, dict) else str(ch) for ch in content)\n",
    "                elif isinstance(content, str):\n",
    "                    text_content = content\n",
    "                else:\n",
    "                    text_content = json.dumps(content)\n",
    "\n",
    "                items.append(\n",
    "                    self.create_text_output_item(\n",
    "                        text=text_content,\n",
    "                        id=message.get(\"id\") or str(uuid4()),\n",
    "                    )\n",
    "                )\n",
    "            elif role == \"tool\":\n",
    "                items.append(\n",
    "                    self.create_function_call_output_item(\n",
    "                        call_id=message.get(\"tool_call_id\"),\n",
    "                        output=message.get(\"content\"),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return items\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        lc_msgs = convert_to_messages(self.prep_msgs_for_cc_llm(request.input))\n",
    "\n",
    "        for event in self.agent.stream({\"messages\": lc_msgs}, stream_mode=[\"updates\", \"messages\"]):\n",
    "            if event[0] == \"updates\":\n",
    "                for node_data in event[1].values():\n",
    "                    for item in self._langchain_to_responses(node_data.get(\"messages\", [])):\n",
    "                        yield ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=item)\n",
    "            elif event[0] == \"messages\":\n",
    "                chunk = event[1][0]\n",
    "                if isinstance(chunk, AIMessageChunk) and (content := chunk.content):\n",
    "                    yield ResponsesAgentStreamEvent(\n",
    "                        **self.create_text_delta(delta=content, item_id=chunk.id),\n",
    "                    )\n",
    "\n",
    "agent = create_tool_calling_agent(base_llm, tools, system_prompt)\n",
    "AGENT = LangGraphResponsesAgent(agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6gjrp4mx6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an actual order_id for input example\n",
    "sample_order_id = spark.sql(f\"\"\"\n",
    "    SELECT order_id \n",
    "    FROM {CATALOG}.lakeflow.all_events \n",
    "    WHERE event_type='delivered'\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0]['order_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ih3tt5qeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sample_order_id is not None\n",
    "print(sample_order_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23o4h7j6amzh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import LLM_ENDPOINT_NAME, tools\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
    "for tool in tools:\n",
    "    resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "# ResponsesAgentRequest format uses \"input\" not \"messages\"\n",
    "input_example = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"My order was really late! Order ID: {sample_order_id}\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"complaint_agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        input_example=input_example,\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
    "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "mlflow.set_active_model(model_id = logged_agent_info.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gfympowd7q",
   "metadata": {},
   "source": [
    "#### Evaluate the Agent\n",
    "\n",
    "- Synthesize diverse complaint scenarios (delivery delays, wrong items, missing items, etc.) from recent orders.\n",
    "- Configure multiple MLflow `Guidelines` scorers that check refund reason, messaging quality, and policy compliance.\n",
    "- Run batch evaluations against the agent to quantify decision quality before promotion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2loovjto96g",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive complaint scenarios for evaluation\n",
    "import random\n",
    "\n",
    "# Get sample order IDs for different scenarios\n",
    "all_order_ids = [\n",
    "    row['order_id'] for row in spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT order_id \n",
    "        FROM {CATALOG}.lakeflow.all_events \n",
    "        WHERE event_type='delivered'\n",
    "        LIMIT 30\n",
    "    \"\"\").collect()\n",
    "]\n",
    "\n",
    "# Create diverse complaint scenarios\n",
    "complaint_scenarios = []\n",
    "\n",
    "# Delivery delay complaints (should get AUTO-CREDIT if truly >P75)\n",
    "for oid in all_order_ids[:5]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My order took forever to arrive! Order ID: {oid}\",\n",
    "        f\"Order was 2 hours late, unacceptable. ID: {oid}\",\n",
    "    ])\n",
    "\n",
    "# Food quality complaints (should be INVESTIGATE, not auto-credit)\n",
    "for oid in all_order_ids[5:10]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My falafel was completely soggy and inedible. Order: {oid}\",\n",
    "        f\"The food was cold when it arrived, very disappointing. Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# Missing items complaints (should be INVESTIGATE)\n",
    "for oid in all_order_ids[10:13]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Half my order was missing - no drinks or sides! Order: {oid}\",\n",
    "        f\"My entire falafel bowl was missing from the order! Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# Service issues (should be INVESTIGATE)\n",
    "for oid in all_order_ids[13:15]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Your driver was extremely rude to me. Order: {oid}\",\n",
    "        f\"Driver left my food in the wrong building. Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# Escalation triggers (should be ESCALATE)\n",
    "for oid in all_order_ids[15:17]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"I'm calling my lawyer about this terrible service! Order: {oid}\",\n",
    "        f\"This food poisoning could have killed me! Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# Sample for reasonable eval size\n",
    "complaint_scenarios = random.sample(complaint_scenarios, min(15, len(complaint_scenarios)))\n",
    "\n",
    "# Wrap in correct input schema for ResponsesAgent\n",
    "data = []\n",
    "for complaint in complaint_scenarios:\n",
    "    data.append({\n",
    "        \"inputs\": {\n",
    "            \"input\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": complaint\n",
    "            }]\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(f\"Created {len(data)} evaluation scenarios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9l6zrp5n03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple scorers and run evaluation\n",
    "\n",
    "from mlflow.genai.scorers import Guidelines\n",
    "import mlflow\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "project_directory = os.path.dirname(notebook_path)\n",
    "sys.path.append(project_directory)\n",
    "\n",
    "from agent import AGENT\n",
    "\n",
    "# Multiple scorers to evaluate different aspects\n",
    "refund_reason = Guidelines(\n",
    "    name=\"refund_reason\",\n",
    "    guidelines=[\n",
    "        \"If a refund is offered, it must clearly relate to the complaint made by the user\",\n",
    "        \"Do not offer timing-related refunds for food quality or missing item complaints\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "decision_quality = Guidelines(\n",
    "    name=\"decision_quality\",\n",
    "    guidelines=[\n",
    "        \"Food quality complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Missing item complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Service complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Legal threats or serious health concerns should be classified as 'escalate'\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "evidence_usage = Guidelines(\n",
    "    name=\"evidence_usage\",\n",
    "    guidelines=[\n",
    "        \"Decisions must be based on actual order data from tool calls\",\n",
    "        \"Credit amounts should be justified by delivery time comparisons to percentiles\",\n",
    "        \"Do not make assumptions without checking order details\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ResponsesAgent predict function wrapper for evaluation\n",
    "# Note: parameter name must match the key in eval data (\"input\")\n",
    "def predict_fn(input):\n",
    "    from mlflow.types.responses import ResponsesAgentRequest\n",
    "    request = ResponsesAgentRequest(input=input)\n",
    "    response = AGENT.predict(request)\n",
    "    # Extract text from ResponsesAgent output structure\n",
    "    # output[0] is an OutputItem, content[0] contains the text\n",
    "    output_item = response.output[-1]  # Get final output (structured response)\n",
    "    if hasattr(output_item, 'content') and output_item.content:\n",
    "        return output_item.content[0][\"text\"]\n",
    "    return str(output_item)\n",
    "\n",
    "# Run evaluation with multiple scorers\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=data,\n",
    "    scorers=[refund_reason, decision_quality, evidence_usage],\n",
    "    predict_fn=predict_fn\n",
    ")\n",
    "\n",
    "print(f\"Evaluation complete. Check MLflow UI for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ttse3kj3pcj",
   "metadata": {},
   "source": [
    "#### Log the agent to `UC`\n",
    "\n",
    "- Point MLflow at the Unity Catalog registry and name the artifact `${CATALOG}.ai.complaint_agent`.\n",
    "- Register the run-produced model so versioned deployments can be promoted through UC stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y8lfco9zzn",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "UC_MODEL_NAME = f\"{CATALOG}.ai.complaint_agent\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "me3m6ovfqkd",
   "metadata": {},
   "source": [
    "#### deploy the agent to model serving\n",
    "\n",
    "- Ensure a production MLflow experiment exists for live trace capture.\n",
    "- Call `agents.deploy` to create/update the Databricks Model Serving endpoint backed by the UC model version.\n",
    "- Wait until the serving endpoint reports READY before continuing to downstream steps.\n",
    "- Pass the prod experiment ID via environment variables so inference logs are persisted automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qjnjjztbbna",
   "metadata": {},
   "outputs": [],
   "source": "import mlflow\n\n# Create prod experiment for production inference traces\n# Use shared path for job compatibility and visibility\nprod_experiment_name = f\"/Shared/{CATALOG}_complaint_agent_prod\"\n\n# set_experiment creates the experiment if it doesn't exist, or activates it if it does\nprod_experiment = mlflow.set_experiment(prod_experiment_name)\nprod_experiment_id = prod_experiment.experiment_id\nprint(f\"✅ Using prod experiment: {prod_experiment_name} (ID: {prod_experiment_id})\")\n\n# Add experiment to UC state for cleanup\nimport sys\nsys.path.append('../utils')\nfrom uc_state import add\n\nexperiment_data = {\n    \"experiment_id\": prod_experiment_id,\n    \"name\": prod_experiment_name\n}\nadd(CATALOG, \"experiments\", experiment_data)\nprint(f\"✅ Added prod experiment to UC state\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exckljo2zx4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from databricks import agents\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointStateReady\n",
    "\n",
    "endpoint_name = dbutils.widgets.get(\"COMPLAINT_AGENT_ENDPOINT_NAME\")\n",
    "deployment_info = agents.deploy(\n",
    "    model_name=UC_MODEL_NAME,\n",
    "    model_version=uc_registered_model_info.version,\n",
    "    scale_to_zero=False,\n",
    "    endpoint_name=endpoint_name,\n",
    "    environment_vars={\"MLFLOW_EXPERIMENT_ID\": str(prod_experiment_id)},\n",
    ")\n",
    "\n",
    "workspace = WorkspaceClient()\n",
    "ready_endpoint = workspace.serving_endpoints.wait_get_serving_endpoint_not_updating(\n",
    "    name=endpoint_name,\n",
    "    timeout=timedelta(minutes=30),\n",
    ")\n",
    "\n",
    "if ready_endpoint.state.ready != EndpointStateReady.READY:\n",
    "    raise RuntimeError(\n",
    "        f\"Endpoint {endpoint_name} is {ready_endpoint.state.ready} after deployment; retry or investigate.\"\n",
    "    )\n",
    "\n",
    "print(f\"✅ Endpoint {endpoint_name} is READY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i1syfkwcivl",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deployment_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4i9yj8vjs2",
   "metadata": {},
   "source": [
    "##### Record model in state\n",
    "\n",
    "- Store the new deployment metadata with `uc_state.add` to facilitate cleanup in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ihnhnv5plw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also add to UC-state\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from uc_state import add\n",
    "\n",
    "add(dbutils.widgets.get(\"CATALOG\"), \"endpoints\", deployment_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bhjj7zuan9g",
   "metadata": {},
   "source": [
    "#### production monitoring\n",
    "\n",
    "- Scaffold MLflow guideline scorers for sampled live traffic to flag decision drift or policy regressions.\n",
    "- Keep monitoring hooks commented until the serving endpoint is stable, then enable to automate QA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v1j0a0y21ct",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import Guidelines, ScorerSamplingConfig\n",
    "\n",
    "# Register scorers for production monitoring (10% sampling)\n",
    "decision_quality_monitor = Guidelines(\n",
    "    name=\"decision_quality_prod\",\n",
    "    guidelines=[\n",
    "        \"Food quality complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Missing item complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Legal threats or serious health concerns should be classified as 'escalate'\"\n",
    "    ]\n",
    ").register(name=f\"{UC_MODEL_NAME}_decision_quality\")\n",
    "\n",
    "refund_reason_monitor = Guidelines(\n",
    "    name=\"refund_reason_prod\",\n",
    "    guidelines=[\n",
    "        \"If a refund is offered, it must clearly relate to the complaint made by the user\"\n",
    "    ]\n",
    ").register(name=f\"{UC_MODEL_NAME}_refund_reason\")\n",
    "\n",
    "# Start monitoring with 10% sampling of production traffic\n",
    "decision_quality_monitor = decision_quality_monitor.start(\n",
    "    sampling_config=ScorerSamplingConfig(sample_rate=0.1)\n",
    ")\n",
    "\n",
    "refund_reason_monitor = refund_reason_monitor.start(\n",
    "    sampling_config=ScorerSamplingConfig(sample_rate=0.1)\n",
    ")\n",
    "\n",
    "print(\"✅ Production monitoring enabled with 10% sampling\")\n",
    "print(f\"   - decision_quality scorer monitoring: {decision_quality_monitor}\")\n",
    "print(f\"   - refund_reason scorer monitoring: {refund_reason_monitor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}